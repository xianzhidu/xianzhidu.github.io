<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xianzhi Du</title>

    <meta name="author" content="Xianzhi Du">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xianzhi Du
                </p>
                <p>I lead the multimodal pre-training effort for Apple Foundation Models at <a href="https://machinelearning.apple.com/">Apple AI/ML</a>. I work on building and training LLM, multimodal LLM, MoE and model scaling.
                </p>
                <p>
                  I was previously a Research Engineer at <a href="https://research.google.com/teams/brain/?authuser=2">Google Brain</a> and Google CoreML working on computer vision research and building <a href="https://github.com/tensorflow/models/tree/master">TensorFlow official models</a>. I also worked extensively on collaborating with Alphabet's teams, including <a href="https://waymo.com/">Waymo</a>, <a href="https://cloud.google.com/?hl=en">Google Cloud</a>, Google Maps, <a href="https://www.google.com/photos/about/">Google Photos</a>, Nest, <a href="https://x.company/">X</a>, to apply state-of-the-art research models to Alphabet's applications.
                </p>
                <p>
                I did my PhD at <a href="https://www.umiacs.umd.edu/">UMIACS</a>, University of Maryland, College Park, where I was advised by <a href="https://www.umiacs.umd.edu/people/lsd">Larry Davis</a> and <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:xzdu025@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=l1hP40AAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/xianzhi-du-1b128934/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://x.com/Phyyysalis">Twitter</a> &nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images2/xianzhidu2.png"><img style="width:100%;max-width:100%;object-fit: cover; " alt="profile photo" src="images2/xianzhidu2.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research Highlights</h2>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/ai2025.png' width=140%>
        </div>
      </td>
      <td style="padding:5px;width:65%;vertical-align:middle">
        <a href="https://machinelearning.apple.com/papers/apple_intelligence_foundation_language_models_tech_report_2025.pdf">
            <span class="papertitle">Apple Intelligence Foundation Language Models 2025
        </a>
        <br>
        <em>arXiv</em>, 2025. Multimodal pre-train lead.
        <br>
        <a href="https://machinelearning.apple.com/research/apple-foundation-models-tech-report-2025">blog post</a>
        /
        <a href="https://github.com/apple/axlearn">Github</a>
        <p></p>
        <p>
        2025 updated version of Apple Intelligence Foundation models. Powering Apple Intelligence features. </a>.
        </p>
      </td>
    </tr>


<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/axlearn.png' width=120%>
        </div>
      </td>
      <td style="padding:5px;width:65%;vertical-align:middle">
        <a href="https://www.arxiv.org/pdf/2507.05411">
            <span class="papertitle">AXLearn: Modular Large Model Training on Heterogeneous Infrastructure
        </a>
        <br>
        <em>arXiv</em>, 2025.
        <br>
        <a href="https://github.com/apple/axlearn">Github</a>
        <p></p>
        <p>
        A production deep learning system that facilitates scalable and high-performance training of large deep learning models </a>.
        </p>
      </td>
    </tr>

<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/ai2.png' width=140%>
        </div>
      </td>
      <td style="padding:5px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2407.21075">
            <span class="papertitle">Apple Intelligence Foundation Language Models
        </a>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://machinelearning.apple.com/research/introducing-apple-foundation-models">blog post</a>
        /
        <a href="https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models">project page</a>
        /
        <a href="https://github.com/apple/axlearn">Github</a>
        <p></p>
        <p>
        Introduces Appleâ€™s on-device and server foundation models. Announced at the <a href=https://developer.apple.com/wwdc24/>2024 Worldwide Developers Conference</a>.
        </p>
      </td>
    </tr>







<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/revisitmoe.png' width=140%>
        </div>
      </td>
      <td style="padding:5px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2405.15052">
			<span class="papertitle">Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training
        </a>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://github.com/apple/axlearn">Github</a>
        <p></p>
        <p>
        Compares MoE and dense LLMs by using step time to measure model speed and designing total train budget with Chinchilla compute-optimal setting. We show MoE consistently outperforms dense models at 6B, 13B and 30B.
        </p>
      </td>
    </tr>








<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/mm1.png' width=120%>
        </div>
      </td>
      <td style="padding:5px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2403.09611">
			<span class="papertitle">MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training
</span>
        </a>
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://machinelearning.apple.com/research/mm1-methods-analysis-insights">project page</a>
        /
        <a href="https://github.com/apple/axlearn">Github</a>
        <p></p>
        <p>
        Builds high-performance MLLMs by identifying crucial architectural and data choices, leading to the creation of MM1 models, which excel in pre-training metrics and few-shot learning across various benchmarks.
        </p>
      </td>
    </tr>



<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/ferret.png' width=140%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2310.07704">
			<span class="papertitle">Ferret: Refer and ground anything anywhere at any granularity
</span>
        </a>
        <br>
        <em>ICLR</em>, 2024 <b><font color="#ff0000">(Spotlight)</font></b>
        <br>
        <a href="https://machinelearning.apple.com/research/ferret">project page</a>
        /
        <a href="https://github.com/apple/ml-ferret">Github</a>
        <p></p>
        <p>
        A novel MLLM that excels in fine-grained spatial understanding and grounding descriptions within images, using a hybrid region representation and a specialized dataset, demonstrating superior performance and reduced object hallucination.
        </p>
      </td>
    </tr>



<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/mgie.png' width=140%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2309.17102">
			<span class="papertitle">Guiding instruction-based image editing via multimodal large language models
</span>
        </a>
        <br>
        <em>ICLR</em>, 2024 <b><font color="#ff0000">(Spotlight)</font></b>
        <br>
        <a href="https://machinelearning.apple.com/research/mgie">project page</a>
        /
        <a href="https://github.com/apple/ml-mgie">Github</a>
        <p></p>
        <p>
        Leverages Multimodal LLMs to enhance instruction-based image editing, derive expressive instructions and provide explicit guidance.
        </p>
      </td>
    </tr>






<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/adamvmoe.png' width=140%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/10377734">
			<span class="papertitle">Adamv-moe: Adaptive multi-task vision mixture-of-experts
</span>
        </a>
        <br>
        <em>ICCV</em>, 2023
        <br>
        <a href="https://github.com/google-research/google-research/tree/master/moe_mtl">Github</a>
        <p></p>
        <p>
        An adaptive MoE framework that dynamically adjusts experts per task, enhancing multi-task vision recognition performance on ImageNet and COCO.
        </p>
      </td>
    </tr>


<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/uvit.png' width=140%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2112.09747">
			<span class="papertitle">A Simple Single-Scale Vision Transformer for Object Detection and Instance Segmentation
</span>
        </a>
        <br>
        <em>ECCV</em>, 2022
        <br>
        <a href="https://github.com/tensorflow/models/tree/master/official">Github</a>
        <p></p>
        <p>
        Designs a simple single-scale vision transformer architecture, achieves strong performance on object localization and instance segmentation tasks.
        </p>
      </td>
    </tr>


<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/resnetrs.png' width=130%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2103.07579">
			<span class="papertitle">Revisiting resnets: Improved training and scaling strategies
</span>
        </a>
        <br>
        <em>Neurips</em>, 2021 <b><font color="#ff0000">(Spotlight)</font></b>
        <br>
        <a href="https://github.com/tensorflow/tpu/tree/master/">Github</a>
        /
        <a href=https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/16>Google Cloud</a>
        /
        <a href=https://wandb.ai/wandb_fc/pytorch-image-models/reports/Revisiting-ResNets-Improved-Training-and-Scaling-Strategies--Vmlldzo2NDE3NTM>Blog post</a>
        <p></p>
        <p>
        Revisits training recipe and model scaling strategies for ResNets, improving ResNets to be competitive with state-of-the-art.
        </p>
      </td>
    </tr>




<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/simpledet.png' width=130%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2107.00057">
			<span class="papertitle">Simple training strategies and model scaling for object detection
</span>
        </a>
        <br>
        <em>arXiv</em>, 2021 </b>
        <br>
        <a href="https://github.com/tensorflow/tpu/tree/master/">Github</a>
        /
        <a href=https://cloud.google.com/tpu/docs/tutorials/retinanet-2.x>Google Cloud</a>
        <p></p>
        <p>
        Revisits training recipe and model scaling strategies for Object Detection, improving conventional object detectors, e.g. RetinaNet and Cascade R-CNN, to be competitive with state-of-the-art.
        </p>
      </td>
    </tr>








<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/spinenet3.png' width=130%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2103.12270">
			<span class="papertitle">Dilated SpineNet for semantic segmentation
</span>
        </a>
        <br>
        <em>arXiv</em>, 2021
        <br>
        <a href="https://github.com/tensorflow/tpu/blob/master/models/official/">Github</a>
        <p></p>
        <p>
        Designs a scale-permuted backbone dilated convolutions that is learned by Neural Architecture Search (NAS) on semantic segmentation. <b>Achieved SoTA</b> on <a href=https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes-val>Cityscape semantic segmentation benchmark<a> on 03/2021.
        </p>
      </td>
    </tr>

<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/spinenet2.png' width=140%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/1912.05027">
			<span class="papertitle">SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization
</span>
        </a>
        <br>
        <em>CVPR</em>, 2020
        <br>
        <a href="https://github.com/tensorflow/tpu/blob/master/models/official/detection/MODEL_ZOO.md#retinanet-trained-from-scratch">Github</a>
        /
        <a href="https://www.youtube.com/watch?v=qFRfnIRMNlk">Yannic Kilcher's Tutorial</a>
        /
        <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/imageobjectdetection-spinenet">Google Cloud</a>
        /
        <a href="https://research.google/blog/spinenet-a-novel-architecture-for-object-detection-discovered-with-neural-architecture-search/">Blog post</a>
        <p></p>
        <p>
        Designs a scale-permuted backbone with intermediate features and cross-scale connections that is learned by Neural Architecture Search (NAS) on object detection. <b>Achieved SoTA</b> on <a href=https://cocodataset.org/#home>COCO detection and segmentation benchmark<a> on 12/2019.
        </p>
      </td>
    </tr>


<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/amnet2.png' width=140%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/1904.09099">
			<span class="papertitle">Amnet: Deep atrous multiscale stereo disparity estimation networks
</span>
        </a>
        <br>
        <em>ICCE</em>, 2020
        <p></p>
        <p>
        Introduces AMNet with depthwise-separable convolutions, extended cost volume, and stacked atrous multiscale network for disparity estimation. <b>Ranked No.1</b> on <a href=https://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo>KITTI Stereo 2015</a> and <a href=https://www.cvlibs.net/datasets/kitti/eval_stereo_flow.php?benchmark=stereo>2012</a> benchmarks on 11/2018.
        </p>
      </td>
    </tr>


<tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:5px;width:35%;vertical-align:middle">
        <div class="one">
          <img src='images2/fdnn.png' width=140%>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/7926694">
			<span class="papertitle">Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection
</span>
        </a>
        <br>
        <em>WACV</em>, 2017
        <p></p>
        <p>
        Proposes a deep neural network fusion architecture for accurate, fast and robust pedestrian detection, especially in detecting small-size and occluded pedestrians. <b>Ranked No.1</b> on <a href=https://data.caltech.edu/records/f6rph-90m20>Caltech Pedestrian Detection benchmark</a> on 08/2016.
        </p>
      </td>
    </tr>















          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <a href=https://github.com/jonbarron/website>Template credits</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
